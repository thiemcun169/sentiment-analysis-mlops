# Sentiment Analysis with TensorRT and Triton

High-performance multilingual sentiment analysis service using NVIDIA TensorRT and Triton Inference Server, wrapped in a FastAPI service and deployed via Docker Compose.

## ğŸš€ Quick Start

### Prerequisites

- NVIDIA GPU with CUDA support
- Docker with NVIDIA Container Toolkit
- Python 3.10+ (for model conversion)
- At least 8GB GPU memory

### 1. Clone and Setup

```bash
git clone <repository>
cd mlops_docker
```

### 2. Convert Model to TensorRT

```bash
# Install conversion dependencies
pip install torch transformers tensorrt onnx

# Convert the model (takes 5-10 minutes)
python convert_to_tensorrt.py
```

### 3. Deploy with Docker Compose

```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f
```

### 4. Test the Service

```bash
# Quick stress test
python docker_stress_test.py --quick

# Full stress test
python docker_stress_test.py
```

## ğŸ“Š Performance Results

### TensorRT Direct Performance (localhost:8000)
- **Peak Throughput**: 1,929 req/s
- **Latency**: 0.9ms - 69ms (P95)
- **Success Rate**: 100%

### FastAPI Service Performance (localhost:8080)
- **Throughput**: ~500-800 req/s (depending on batch size)
- **Features**: Batch processing, validation, error handling
- **Endpoints**: `/predict`, `/predict/single`, `/health`

## ğŸ› ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚    â”‚   Triton        â”‚    â”‚   TensorRT      â”‚
â”‚   Service       â”‚â”€â”€â”€â–¶â”‚   Server        â”‚â”€â”€â”€â–¶â”‚   Engine        â”‚
â”‚   (Port 8080)   â”‚gRPCâ”‚   (Port 8001)   â”‚    â”‚   (.plan)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

1. **TensorRT Engine**: Optimized sentiment analysis model
   - Model: `tabularisai/multilingual-sentiment-analysis`
   - Format: TensorRT engine (.plan file)
   - Optimization: FP16 precision
   - Classes: 5 (very_negative, negative, neutral, positive, very_positive)

2. **Triton Inference Server**: Model serving platform
   - Backend: TensorRT
   - Batch size: 1-4
   - Dynamic batching enabled
   - GPU acceleration

3. **FastAPI Service**: REST API wrapper
   - Endpoints: `/predict`, `/predict/single`, `/health`, `/model/info`
   - Validation: Input validation and error handling
   - Communication: gRPC with Triton
   - Features: Batch processing, detailed responses

## ğŸ”§ Configuration

### Model Configuration (`triton-model-repository/sentiment_analysis_tensorrt/config.pbtxt`)

```protobuf
name: "sentiment_analysis_tensorrt"
backend: "tensorrt"
max_batch_size: 4
platform: "tensorrt_plan"

input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [ 128 ]
  },
  {
    name: "attention_mask"
    data_type: TYPE_INT64
    dims: [ 128 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 5 ]
  }
]
```

### Docker Compose Services

- **triton-server**: NVIDIA Triton container with GPU access
- **fastapi-service**: Custom FastAPI container
- **Networks**: Custom bridge network for service communication

## ğŸ“¡ API Usage

### Predict Multiple Texts

```bash
curl -X POST "http://localhost:8080/predict" \
     -H "Content-Type: application/json" \
     -d '{
       "texts": [
         "I love this product!",
         "This is terrible quality."
       ],
       "return_scores": true
     }'
```

### Predict Single Text

```bash
curl -X POST "http://localhost:8080/predict/single?text=Great product!&return_scores=true"
```

### Health Check

```bash
curl http://localhost:8080/health
```

### Model Information

```bash
curl http://localhost:8080/model/info
```

## ğŸ“ˆ Performance Testing

### Available Test Scripts

1. **Direct Triton Testing**: `simple_stress_test.py`
   - Tests Triton server directly
   - Maximum performance measurement
   - Use for baseline performance

2. **Docker Compose Testing**: `docker_stress_test.py`
   - Tests complete FastAPI + Triton stack
   - Real-world performance measurement
   - Use for production validation

### Test Options

```bash
# Quick test (reduced load)
python docker_stress_test.py --quick

# Custom test parameters
python docker_stress_test.py \
  --light-requests 50 \
  --medium-requests 200 \
  --heavy-requests 500 \
  --heavy-concurrency 25

# Direct Triton test
python simple_stress_test.py --quick
```

## ğŸ›¡ï¸ Production Considerations

### Security
- Add authentication/authorization
- Enable HTTPS/TLS
- Implement rate limiting
- Add input sanitization

### Monitoring
- Add metrics collection (Prometheus)
- Implement logging aggregation
- Set up health checks
- Monitor GPU utilization

### Scaling
- Use multiple Triton instances
- Implement load balancing
- Add horizontal pod autoscaling (Kubernetes)
- Consider model versioning

## ğŸ” Troubleshooting

### Common Issues

1. **TensorRT Version Mismatch**
   ```bash
   # Check TensorRT version
   python -c "import tensorrt; print(tensorrt.__version__)"
   
   # Must match Triton container (10.0.1)
   pip install tensorrt==10.0.1
   ```

2. **Model Not Loading**
   ```bash
   # Check model files
   ls -la triton-model-repository/sentiment_analysis_tensorrt/
   
   # Check Triton logs
   docker-compose logs triton-server
   ```

3. **FastAPI Connection Issues**
   ```bash
   # Check network connectivity
   docker-compose exec fastapi-service ping triton
   
   # Check service logs
   docker-compose logs fastapi-service
   ```

### Performance Optimization

1. **GPU Memory**: Ensure sufficient GPU memory (8GB+ recommended)
2. **Batch Size**: Adjust max_batch_size in model config
3. **Concurrency**: Tune FastAPI worker count
4. **Network**: Use dedicated network for high throughput

## ğŸ“ Project Structure

```
mlops_docker/
â”œâ”€â”€ convert_to_tensorrt.py         # Model conversion script
â”œâ”€â”€ docker-compose.yml             # Docker Compose configuration
â”œâ”€â”€ simple_stress_test.py          # Direct Triton stress test
â”œâ”€â”€ docker_stress_test.py          # Docker Compose stress test
â”œâ”€â”€ fastapi-service/               # FastAPI service
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ main.py               # FastAPI application
â”œâ”€â”€ triton-model-repository/       # Triton model repository
â”‚   â””â”€â”€ sentiment_analysis_tensorrt/
â”‚       â”œâ”€â”€ config.pbtxt          # Model configuration
â”‚       â”œâ”€â”€ tokenizer_info.json   # Tokenizer metadata
â”‚       â””â”€â”€ 1/
â”‚           â””â”€â”€ model.plan        # TensorRT engine
â””â”€â”€ logs/                         # Application logs
```

## ğŸ¯ Use Cases

### Real-time Sentiment Analysis
- Social media monitoring
- Customer feedback analysis
- Product review processing
- Chat/comment moderation

### Batch Processing
- Large-scale text analysis
- Historical data processing
- Report generation
- Data pipeline integration

## ğŸ“Š Benchmarks

### Hardware Used
- GPU: NVIDIA RTX 4090 (24GB VRAM)
- CPU: Intel i9-13900K
- RAM: 64GB DDR5

### Results Summary
- **Best Throughput**: 1,929 req/s (Direct Triton)
- **Production Throughput**: ~600 req/s (FastAPI + Triton)
- **Latency P95**: <70ms
- **GPU Utilization**: ~30-50% under load
- **Memory Usage**: ~2-3GB VRAM

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- NVIDIA Triton Inference Server team
- HuggingFace Transformers team
- FastAPI development team
- `tabularisai/multilingual-sentiment-analysis` model authors